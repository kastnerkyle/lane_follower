<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_8nesm8brh87b-0>li:before{content:"\0025cf  "}.lst-kix_8nesm8brh87b-2>li:before{content:"\0025a0  "}.lst-kix_8nesm8brh87b-1>li:before{content:"\0025cb  "}ul.lst-kix_jki0ddmij0sf-8{list-style-type:none}.lst-kix_8nesm8brh87b-6>li:before{content:"\0025cf  "}ul.lst-kix_jki0ddmij0sf-5{list-style-type:none}ul.lst-kix_jki0ddmij0sf-4{list-style-type:none}ul.lst-kix_jki0ddmij0sf-7{list-style-type:none}.lst-kix_8nesm8brh87b-7>li:before{content:"\0025cb  "}.lst-kix_jki0ddmij0sf-3>li:before{content:"\0025cf  "}ul.lst-kix_jki0ddmij0sf-6{list-style-type:none}.lst-kix_jki0ddmij0sf-2>li:before{content:"\0025a0  "}.lst-kix_jki0ddmij0sf-1>li:before{content:"\0025cb  "}.lst-kix_8nesm8brh87b-5>li:before{content:"\0025a0  "}ul.lst-kix_8nesm8brh87b-1{list-style-type:none}.lst-kix_jki0ddmij0sf-0>li:before{content:"\0025cf  "}ul.lst-kix_8nesm8brh87b-2{list-style-type:none}ul.lst-kix_8nesm8brh87b-0{list-style-type:none}.lst-kix_8nesm8brh87b-4>li:before{content:"\0025cb  "}ul.lst-kix_8nesm8brh87b-5{list-style-type:none}ul.lst-kix_8nesm8brh87b-6{list-style-type:none}ul.lst-kix_8nesm8brh87b-3{list-style-type:none}.lst-kix_8nesm8brh87b-3>li:before{content:"\0025cf  "}ul.lst-kix_8nesm8brh87b-4{list-style-type:none}ul.lst-kix_8nesm8brh87b-7{list-style-type:none}ul.lst-kix_8nesm8brh87b-8{list-style-type:none}ul.lst-kix_jki0ddmij0sf-1{list-style-type:none}ul.lst-kix_jki0ddmij0sf-0{list-style-type:none}ul.lst-kix_jki0ddmij0sf-3{list-style-type:none}.lst-kix_8nesm8brh87b-8>li:before{content:"\0025a0  "}ul.lst-kix_jki0ddmij0sf-2{list-style-type:none}.lst-kix_jki0ddmij0sf-4>li:before{content:"\0025cb  "}.lst-kix_jki0ddmij0sf-5>li:before{content:"\0025a0  "}.lst-kix_jki0ddmij0sf-6>li:before{content:"\0025cf  "}.lst-kix_jki0ddmij0sf-8>li:before{content:"\0025a0  "}.lst-kix_jki0ddmij0sf-7>li:before{content:"\0025cb  "}ol{margin:0;padding:0}table td,table th{padding:0}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c3{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c10{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c14{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:center}.c2{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c12{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c7{color:inherit;text-decoration:inherit}.c5{color:#1155cc;text-decoration:underline}.c9{margin-left:36pt;padding-left:0pt}.c8{padding:0;margin:0}.c11{margin-left:72pt;padding-left:0pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c12"><div><p class="c0"><span class="c4"></span></p></div><h1 class="c10" id="h.y87jlf73e407"><span class="c13">Autonomous Q-learning Line Following Robot</span></h1><p class="c1"><span class="c5"><a class="c7" href="https://www.google.com/url?q=https://github.com/YuzhongHuang&amp;sa=D&amp;ust=1491094348855000&amp;usg=AFQjCNF0yTIpmER9GPF7U9TLBxmCLpHD-w">Yuzhong Huang</a></span><span>, </span><span class="c5"><a class="c7" href="https://www.google.com/url?q=https://github.com/NathanYee&amp;sa=D&amp;ust=1491094348856000&amp;usg=AFQjCNHq_C_A-OFWjBiOhcfe9x_0dcPd3w">Nathan Yee</a></span><span>, </span><span class="c5"><a class="c7" href="https://www.google.com/url?q=https://github.com/kzhang8850&amp;sa=D&amp;ust=1491094348857000&amp;usg=AFQjCNFPhr5UpoBVa3FOVe68mvrU1glgdw">Kevin Zhang</a></span></p><h2 class="c14" id="h.mzc27jq8923h"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 465.50px; height: 343.90px;"><img alt="" src="images/image00.jpg" style="width: 465.50px; height: 343.90px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><h2 class="c2" id="h.2royvpe1s042"><span>Project Goal</span></h2><p class="c1"><span>Our project demonstrates that a robot can learn to follow lines in real time with a combination of computer vision, Q-learning, and convolutional neural networks.</span></p><h2 class="c2" id="h.ibf8bxs4iee1"><span>How it&rsquo;s done.</span></h2><p class="c1"><span>The main engine of the project consists of a two layer convolutional neural network attached to a Q-learning (Reinforced Learning) algorithm. There are three main components to the system: the Image Processor, the Network, and the Actuator. The Image Processor takes in images from the Neato&rsquo;s camera, resizes them into a 32x32 image and then converts them into a binary images that filters specifically for the red tape line. Each image is then used as an input to the Neural Network, where it passes through the 2 layers of the convolutional layers and then through the Q-Learning algorithm to generate probability values for each possible action. The Network will then output the most probable action based on the probabilities to the Actuator, which will then move Neato. After the Neato has moved, the Image Processor takes a new picture that &nbsp;is put back into the Network to update the probability values of the previous step. In this manner the Network &ldquo;learns&rdquo;, and over many iterations the probability space becomes optimized to follow the line.</span></p><h2 class="c2" id="h.y6liflxxb5fv"><span class="c3">Design Decisions</span></h2><p class="c1"><span class="c4">One decision we made was to use Tensorflow over Keras as our primary Learning package. Keras wraps around Tensorflow and makes things nice to work with, but we wanted to learn more about Reinforced Learning and how it works, so we decided to go with TensorFlow because it&rsquo;s more low-level and offers more insight into how Q-Learning and layers function together.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">Another decision was to make a two layer Convolutional Neural Network over a single layer Neural Network. The two layer ConvNet is more complicated, but for the purposes of image processing, it should converge much faster and lead to more accurate results. Our original single fully connected layer first was not able to learn fast enough. The additional two layer convolutional layers are just sophisticated enough such that our learning was tangible and accurate.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">We also made some decisions on the reward function. Originally we decided to look at the entire binary image and count the number of white pixels in the 640x480 image - linearly weighting the bottom pixels to be more than the top pixels, but this method didn&rsquo;t allow for negative reinforcement. Next, we tried simplifying the image and went for a function that just looked at the bottom center 30 pixels of a 32x32 image and counted how many were white, with a threshold determining reward and punishment. However, this was too rigid and difficult to properly tune. We ended up using a function that looked at the bottom half of the 32x32 image and counted the number of white pixels, then it compared that with the previous reward output to determine the reward or punishment depending on whether it was higher or lower than the previously recorded value. There are still some loopholes, but for our purposes this was accurate enough.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">Another design decision we made is to take out the dropout layer in the convolutional neural network since dropout layers makes the network unstable. One possible explanation is that in supervised learning, mini-batches increase the complexity of data, and adding noise can help reduce overfitting without adding too much instability to the network. Reinforcement Learning back-propagates only a single state data each iteration, so adding noise without much complexity will make the network unstable. </span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span>For the input of the neural network</span><span>&nbsp;</span><span>we chose to convert our original image from an RGB </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 301.86px;"><img alt="" src="images/image02.png" style="width: 624.00px; height: 301.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>640x480 pixel image to a binary 32x32 image. This ultimately allows us to evaluate and train our neural network faster and reduce the complexity of the network. </span></p><h2 class="c2" id="h.du6vxzb6n5ch"><span>Code Structure</span></h2><p class="c1"><span>Class </span><span class="c4">RobotController:</span></p><ul class="c8 lst-kix_8nesm8brh87b-0 start"><li class="c1 c9"><span class="c4">Subscribers</span></li></ul><ul class="c8 lst-kix_8nesm8brh87b-1 start"><li class="c1 c11"><span class="c4">/camera/image_raw - inherit from ImageSubscriber </span></li></ul><ul class="c8 lst-kix_8nesm8brh87b-0"><li class="c1 c9"><span class="c4">Publishers</span></li></ul><ul class="c8 lst-kix_8nesm8brh87b-1 start"><li class="c1 c11"><span class="c4">/cmd_vel - inherit from CmdVelPublisher</span></li></ul><ul class="c8 lst-kix_8nesm8brh87b-0"><li class="c1 c9"><span class="c4">Attributes</span></li></ul><ul class="c8 lst-kix_8nesm8brh87b-1 start"><li class="c1 c11"><span class="c4">DQN (Deep Q Learning Network) object</span></li></ul><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">RobotControl control loop:</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 594.50px; height: 361.46px;"><img alt="" src="images/image01.png" style="width: 718.93px; height: 538.74px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c2" id="h.6rf9thqraozr"><span class="c3">Challenges</span></h2><p class="c1"><span class="c4">Reward functions make or break reinforcement learning. A bad reward function makes it impossible for the network to learn the proper behavior. A good reward function needs to encourage good behavior without being explicit. It took us several iterations to create a sufficient reward function that allowed for proper learning. &nbsp;We choose to score images by counting the number of white pixels in the image before and after the chosen action. If the before image had a higher score than the after image, we gave a score of -1 to the network. If the first image had the same score as the after image, we gave a score of 0 to the network. If the before image has a higher score than the after image we have a score of +1 to the network. Given this very generic non problem specific reward function, our network is able learn to follow the line. See design decisions for the other reward function we tried.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span>Learning rate explosion can be caught early by printing the output vector during the control loop. There was a time when we realized the Neato kept losing its focus on the line after a short period every run. After many attempts to debug, we finally printed our Q-values and found that they had skyrocketed exponentially in less than 2 seconds. This helped determine that the learning rate is too high when the output vector displays [nan, nan, nan] after three training cycles.</span></p><h2 class="c2" id="h.6xrf5vtf3tla"><span class="c3">Future Work</span></h2><p class="c1"><span>Right now our robot only takes discrete states and outputs discrete actions. In the future, we hope to implement policy gradient or normalized advantage functions to enable continuous learning.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">Further optimizing our Q-learning algorithm could also be a thing, as right now it moves pretty slowly or sometimes gets confused, which might be due to some overfitting or lack of proper parameter tuning.</span></p><h2 class="c2" id="h.vbm163lxj8ov"><span class="c3">Lessons Learned</span></h2><p class="c1"><span class="c4">Reinforcement Learning, or machine learning in general, is just a bunch of vectors being operated on. The &ldquo;learning&rdquo; is really just tweaking the vectors and &ldquo;Q-value&rdquo; probabilities that determine where to move and change based on what it sees after it moves.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">A good reward function is critical to the successful convergence of the neural network.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">Preprocessing input images can lead to faster convergence of the neural network.</span></p><p class="c0"><span class="c4"></span></p><p class="c1"><span class="c4">There are numerous ways to implement a Neural Network or tweak it, and whether a particular way works better for you depends highly on the input you&rsquo;re giving the network, more generalized to being what goal you&rsquo;re trying to achieve.</span></p><p class="c0"><span class="c4"></span></p><h2 class="c2" id="h.sr3ioqk5hp1l"><span class="c3">Videos</span></h2><p class="c1"><span class="c5"><a class="c7" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DR__f9THwd-A%26feature%3Dyoutu.be&amp;sa=D&amp;ust=1491094348888000&amp;usg=AFQjCNEvICeA-3qVtqK34yXwNPtAbQt_GA">Demonstration 1</a></span></p></body></html>